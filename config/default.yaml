# HOLO-1.5 VoxSigil Library Configuration
# Central configuration file for all HOLO-1.5 ensemble components

# === Ensemble Configuration ===
ensemble:
  # Core ensemble settings
  name: "holo15_arc_ensemble"
  version: "1.5.0"
  mode: "production"  # development, testing, production
  
  # Device and resource settings
  device: "auto"  # auto, cpu, cuda, cuda:0, etc.
  mixed_precision: true
  memory_efficient: true
  max_gpu_memory_gb: 4.0
  
  # Ensemble composition
  agents:
    splr_encoder: true
    akorn_binder: true
    lnu_reasoner: true
    gnn_reasoner: true
    meta_controller: true
  
  # Pipeline configuration
  pipeline:
    parallel_processing: true
    max_concurrent_tasks: 4
    timeout_seconds: 30.0
    fallback_strategy: "graceful_degradation"

# === Novel Paradigms Configuration ===
paradigms:
  # Memory optimization paradigms
  efficiency:
    minicache:
      enabled: true
      compression_ratio: 0.7
      similarity_threshold: 0.95
      cache_size_mb: 512
    
    deltanet_attention:
      enabled: true
      linear_complexity: true
      delta_rule_strength: 0.8
      head_reduction_factor: 0.5
    
    adaptive_memory:
      enabled: true
      dynamic_allocation: true
      memory_pressure_threshold: 0.85
      cleanup_interval_seconds: 60
  
  # Reasoning paradigms
  reasoning:
    logical_neural_units:
      enabled: true
      fuzzy_logic: true
      symbolic_reasoning: true
      logic_depth: 4
    
    kuramoto_neurons:
      enabled: true
      oscillator_count: 128
      coupling_strength: 0.3
      synchronization_threshold: 0.8
    
    spiking_networks:
      enabled: true
      spike_threshold: 0.5
      refractory_period_ms: 2.0
      splr_integration: true
    
    relation_patterns:
      enabled: true
      equality_detection: true
      bayesian_priors: true
      differential_rectifiers: true
    
    graph_reasoning:
      enabled: true
      max_nodes: 1000
      edge_features: 64
      gnn_layers: 3

# === Meta-Control Configuration ===
meta_control:
  # Effort paradox mitigation
  effort_controller:
    enabled: true
    complexity_scaling: "adaptive"
    effort_threshold: 0.7
    paradox_detection: true
  
  # Complexity monitoring
  complexity_monitor:
    enabled: true
    real_time_assessment: true
    complexity_levels: ["trivial", "moderate", "complex", "extremely_complex"]
    adaptation_speed: 0.1
  
  # Resource allocation
  resource_allocator:
    enabled: true
    dynamic_budgeting: true
    priority_scheduling: true
    load_balancing: true

# === Training Configuration ===
training:
  # Sleep Training Cycle (STC) settings
  stc:
    enabled: false  # Usually disabled in production
    cycle_interval_hours: 24
    max_training_hours: 8
    validation_frequency: 100
    early_stopping: true
  
  # Canary grid validation
  canary_validation:
    enabled: true
    accuracy_threshold: 0.85
    degradation_threshold: 0.05
    pattern_count: 10
    validation_interval_hours: 1
  
  # Hyperparameter optimization
  hyperparameter_search:
    enabled: false
    search_space: "adaptive"
    max_trials: 50
    optimization_metric: "accuracy"

# === Monitoring & Telemetry ===
monitoring:
  # Prometheus metrics
  metrics:
    enabled: true
    collection_interval_seconds: 10
    export_port: 8000
    export_host: "0.0.0.0"
    detailed_traces: true
  
  # Performance monitoring
  performance:
    latency_tracking: true
    memory_tracking: true
    gpu_monitoring: true
    cognitive_load_tracking: true
  
  # Health checks
  health:
    enabled: true
    check_interval_seconds: 30
    failure_threshold: 3
    recovery_timeout_seconds: 300

# === Logging Configuration ===
logging:
  # General logging
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # File logging
  file_logging:
    enabled: true
    log_directory: "logs"
    max_file_size_mb: 100
    backup_count: 5
    rotation: "daily"
  
  # Reasoning traces
  reasoning_traces:
    enabled: true
    trace_directory: "logs/reasoning_traces"
    max_trace_files: 1000
    detailed_proofs: true
    minimal_slices: true

# === Safety & Security ===
safety:
  # Canary grid safety
  canary_grid:
    enabled: true
    abort_on_degradation: true
    alert_threshold: 0.1
    notification_webhook: null
  
  # Model integrity
  model_integrity:
    checksum_validation: true
    signature_verification: false
    tamper_detection: true
  
  # Resource limits
  resource_limits:
    max_memory_gb: 8.0
    max_gpu_memory_gb: 4.0
    max_cpu_cores: 8
    max_inference_time_seconds: 60

# === Data Configuration ===
data:
  # Dataset management
  datasets:
    arc_training: "data/arc/training"
    arc_evaluation: "data/arc/evaluation"
    synthetic_tasks: "data/synthetic"
    canary_patterns: "data/canary"
  
  # Data preprocessing
  preprocessing:
    normalize_grids: true
    augment_data: true
    cache_preprocessed: true
    max_grid_size: 30
  
  # Data loading
  loading:
    batch_size: 16
    num_workers: 4
    prefetch_factor: 2
    pin_memory: true

# === Shadow Mode Configuration ===
shadow_mode:
  # Shadow deployment settings
  enabled: false  # Set to true for shadow mode
  sample_rate: 1.0  # Fraction of traffic to shadow
  
  # Performance thresholds
  thresholds:
    max_latency_increase_percent: 50.0
    max_memory_increase_mb: 1024.0
    min_accuracy_match_rate: 0.95
  
  # Logging and analysis
  logging:
    detailed_logging: true
    log_directory: "logs/shadow_mode"
    comparison_timeout_seconds: 30.0

# === API Configuration ===
api:
  # REST API settings
  rest:
    enabled: false
    host: "0.0.0.0"
    port: 8080
    cors_enabled: true
    rate_limiting: true
  
  # GraphQL API settings
  graphql:
    enabled: false
    endpoint: "/graphql"
    introspection: false
    playground: false
  
  # Authentication
  auth:
    enabled: false
    method: "api_key"  # api_key, jwt, oauth
    key_expiration_hours: 24

# === Environment-Specific Overrides ===
environments:
  development:
    ensemble.mode: "development"
    logging.level: "DEBUG"
    logging.reasoning_traces.detailed_proofs: true
    monitoring.metrics.collection_interval_seconds: 5
    safety.resource_limits.max_inference_time_seconds: 120
  
  testing:
    ensemble.mode: "testing"
    training.canary_validation.validation_interval_hours: 0.1
    paradigms.efficiency.minicache.cache_size_mb: 256
    data.loading.batch_size: 8
  
  production:
    ensemble.mode: "production"
    logging.level: "INFO"
    monitoring.health.enabled: true
    safety.canary_grid.abort_on_degradation: true
    shadow_mode.enabled: false

# === CLI Configuration ===
cli:
  # Default command settings
  defaults:
    config_file: "config/default.yaml"
    log_level: "INFO"
    device: "auto"
    output_format: "json"
  
  # Command aliases
  aliases:
    train: "python -m voxsigil.training.train"
    infer: "python -m voxsigil.inference.run"
    validate: "python -m voxsigil.validation.validate"
    monitor: "python -m voxsigil.monitoring.dashboard"

# === Integration Configuration ===
integrations:
  # External services
  external:
    wandb:
      enabled: false
      project: "holo15-arc"
      entity: null
    
    tensorboard:
      enabled: true
      log_dir: "logs/tensorboard"
    
    mlflow:
      enabled: false
      tracking_uri: null
  
  # Notification services
  notifications:
    slack:
      enabled: false
      webhook_url: null
      channel: "#holo15-alerts"
    
    email:
      enabled: false
      smtp_server: null
      port: 587
      username: null
      password: null

# === Version Information ===
version:
  holo: "1.5.0"
  config_schema: "1.0.0"
  last_updated: "2025-06-12T00:00:00Z"
  compatibility:
    min_python: "3.8"
    max_python: "3.11"
    pytorch_min: "1.13.0"
    cuda_min: "11.0"
